#!/usr/bin/env python3
"""
BULLETPROOF CHUNKING MODULE
Standalone chunking system that achieves 100% coverage for any Project Gutenberg book.
Hybrid AI + Python approach with guaranteed reliability.
"""

import json
import re
from utils import get_openai_client

def ai_chapter_analyzer(text: str, model: str = "gpt-4.1-nano") -> dict:
    """AI analyzes the book structure and identifies the exact chapter pattern.
    
    Args:
        text: The full text to analyze
        model: The AI model to use
        
    Returns:
        Dictionary with chapter pattern and recommended chunk sizes
    """
    print("üïµÔ∏è AI CHAPTER ANALYZER: Finding the perfect chapter pattern...")
    
    system_prompt = """You are an expert at analyzing Project Gutenberg book structures.
    Your job is to identify the EXACT chapter pattern used in this book.
    
    CRITICAL REQUIREMENTS:
    1. Identify the precise regex pattern to find ALL chapters
    2. Count the total number of chapters
    3. Recommend how many chapters to group together (target ~15k tokens per chunk)
    
    Return JSON with:
    - chapter_regex: exact regex pattern to match chapters
    - total_chapters: total count of chapters found
    - chapters_per_chunk: recommended chapters per chunk for ~15k tokens
    - sample_chapters: first 3 chapter titles found
    """
    
    user_prompt = f"""Analyze this Project Gutenberg book and find its chapter pattern.
    
    Look for patterns like:
    - "CHAPTER I", "CHAPTER II" (Roman numerals)
    - "Chapter 1", "Chapter 2" (Arabic numbers)
    - "CHAPTER ONE", "CHAPTER TWO" (spelled out)
    
    Find the EXACT pattern and count ALL chapters in the book.
    
    Text to analyze:
    {text}
    
    Return the analysis as JSON."""
    
    try:
        client = get_openai_client()
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            response_format={"type": "json_object"},
            temperature=0
        )
        
        result = json.loads(response.choices[0].message.content)
        
        if 'chapter_regex' in result and 'total_chapters' in result:
            print(f"‚úÖ AI found pattern: {result['chapter_regex']}")
            print(f"‚úÖ Total chapters: {result['total_chapters']}")
            print(f"‚úÖ Recommended chapters per chunk: {result.get('chapters_per_chunk', 'unknown')}")
            return result
        else:
            print("‚ùå AI response missing required fields")
            return {}
            
    except Exception as e:
        print(f"‚ùå AI analyzer failed: {e}")
        return {}

def python_pattern_detector(text: str) -> dict:
    """Python fallback to detect chapter patterns when AI is unavailable."""
    print("üîß PYTHON PATTERN DETECTOR: Scanning for chapter patterns...")
    
    # Test common patterns and find the best match
    patterns = [
        (r'CHAPTER [LXIVCDM]+\.?\s*$', 'CHAPTER + Roman (CHAPTER I, II, III...)'),
        (r'CHAPTER \d+\.?\s*$', 'CHAPTER + Number (CHAPTER 1, 2, 3...)'),
        (r'CHAPTER \d+\..*', 'CHAPTER + Number + Title (CHAPTER 1. Title)'),  # Moby Dick pattern
        (r'Chapter [LXIVCDM]+\.?\s*$', 'Chapter + Roman (Chapter I, II, III...)'),
        (r'Chapter \d+\.?\s*$', 'Chapter + Number (Chapter 1, 2, 3...)'),
        (r'Chapter \d+\..*', 'Chapter + Number + Title (Chapter 1. Title)'),
        (r'^[LXIVCDM]+\.\s*$', 'Roman only (I., II., III....)'),
        (r'^\d+\.\s*$', 'Number only (1., 2., 3....)'),
    ]
    
    best_pattern = None
    best_count = 0
    best_description = ""
    
    for pattern, description in patterns:
        matches = re.findall(pattern, text, re.MULTILINE | re.IGNORECASE)
        count = len(matches)
        
        if count > best_count:
            best_pattern = pattern
            best_count = count
            best_description = description
            
        print(f"  Pattern '{description}': {count} matches")
    
    if best_count == 0:
        print("‚ùå No chapter pattern found!")
        return {}
    
    print(f"üéØ Best pattern: {best_description} ({best_count} chapters)")
    
    # Estimate chapters per chunk (target ~15k tokens = ~60k chars)
    # Estimate: total_text / total_chapters = avg_chapter_size
    avg_chapter_size = len(text) // best_count if best_count > 0 else 10000
    
    # Be more conservative with larger chapters
    if avg_chapter_size > 10000:  # Large chapters like Moby Dick
        target_chunk_size = 50000  # Smaller target for safety
    else:
        target_chunk_size = 60000  # Normal target
        
    chapters_per_chunk = max(1, target_chunk_size // avg_chapter_size)
    
    print(f"üìä Size analysis: avg_chapter={avg_chapter_size:,} chars, target_chunk={target_chunk_size:,}, chapters_per_chunk={chapters_per_chunk}")
    
    return {
        'chapter_regex': best_pattern,
        'total_chapters': best_count,
        'chapters_per_chunk': chapters_per_chunk,
        'sample_chapters': ['Detected with Python fallback']
    }

def bulletproof_chunker(text: str, analysis: dict) -> list:
    """Creates chunks with guaranteed 100% text coverage.
    
    Args:
        text: The full text to chunk
        analysis: Analysis dict with chapter_regex and chapters_per_chunk
        
    Returns:
        List of chunk dictionaries with guaranteed 100% coverage
    """
    print("‚öôÔ∏è BULLETPROOF CHUNKER: Creating 100% coverage chunks...")
    
    chapter_regex = analysis.get('chapter_regex', '')
    chapters_per_chunk = analysis.get('chapters_per_chunk', 10)
    total_chapters = analysis.get('total_chapters', 0)
    
    if not chapter_regex:
        print("‚ùå No chapter pattern provided")
        return []
    
    # Find ALL chapter positions
    chapter_matches = list(re.finditer(chapter_regex, text, re.MULTILINE | re.IGNORECASE))
    actual_chapters = len(chapter_matches)
    
    print(f"üîç Found {actual_chapters} chapters (expected {total_chapters})")
    
    if actual_chapters == 0:
        print("‚ùå No chapters found with the pattern!")
        return []
    
    chunks = []
    
    # STEP 1: Include everything BEFORE first chapter (prologue/front matter)
    first_chapter_pos = chapter_matches[0].start()
    if first_chapter_pos > 0:
        prologue = text[:first_chapter_pos]  # Don't strip!
        if prologue:
            chunks.append({
                'text': prologue,
                'description': 'Front matter and prologue',
                'chapters': 'prologue',
                'size': len(prologue),
                'start_pos': 0,
                'end_pos': first_chapter_pos
            })
            print(f"‚úÖ Prologue: {len(prologue):,} characters")
    
    # STEP 2: Group chapters into chunks  
    for i in range(0, actual_chapters, chapters_per_chunk):
        start_chapter_idx = i
        end_chapter_idx = min(i + chapters_per_chunk - 1, actual_chapters - 1)
        
        # Get text from start of first chapter to start of next group (or end of text)
        chunk_start = chapter_matches[start_chapter_idx].start()
        
        if i + chapters_per_chunk < actual_chapters:
            # Not the last chunk - go to start of next chapter group
            chunk_end = chapter_matches[i + chapters_per_chunk].start()
        else:
            # Last chunk - go to absolute end of text to ensure 100% coverage
            chunk_end = len(text)
        
        chunk_text = text[chunk_start:chunk_end]  # Don't strip!
        
        # BULLETPROOF SIZE CHECK - split if too large
        MAX_CHUNK_SIZE = 80000  # 80k chars = ~20k tokens (safe for 32k output)
        
        if len(chunk_text) > MAX_CHUNK_SIZE:
            print(f"‚ö†Ô∏è Chunk too large ({len(chunk_text):,} chars), splitting...")
            
            # Split this group into smaller chunks (1 chapter each if needed)
            for single_idx in range(start_chapter_idx, end_chapter_idx + 1):
                single_start = chapter_matches[single_idx].start()
                
                if single_idx + 1 < actual_chapters:
                    single_end = chapter_matches[single_idx + 1].start()
                else:
                    single_end = len(text)
                
                single_text = text[single_start:single_end]
                single_num = single_idx + 1
                
                chunks.append({
                    'text': single_text,
                    'description': f'Chapter {single_num}',
                    'chapters': f'{single_num}',
                    'size': len(single_text),
                    'start_pos': single_start,
                    'end_pos': single_end
                })
                
                print(f"‚úÖ Split chunk {len(chunks)}: Chapter {single_num} = {len(single_text):,} chars")
        else:
            start_chapter_num = start_chapter_idx + 1
            end_chapter_num = end_chapter_idx + 1
            
            chunks.append({
                'text': chunk_text,
                'description': f'Chapters {start_chapter_num}-{end_chapter_num}',
                'chapters': f'{start_chapter_num}-{end_chapter_num}',
                'size': len(chunk_text),
                'start_pos': chunk_start,
                'end_pos': chunk_end
            })
            
            print(f"‚úÖ Chunk {len(chunks)}: Chapters {start_chapter_num}-{end_chapter_num} = {len(chunk_text):,} chars")
    
    # VERIFY 100% COVERAGE
    total_chunk_size = sum(chunk['size'] for chunk in chunks)
    coverage = (total_chunk_size / len(text)) * 100
    
    print(f"\nüìä COVERAGE VERIFICATION:")
    print(f"   Original text: {len(text):,} characters")
    print(f"   Chunked text: {total_chunk_size:,} characters")
    print(f"   Coverage: {coverage:.1f}%")
    
    if coverage >= 99.995:
        print("‚úÖ 100% COVERAGE ACHIEVED!")
    else:
        print("‚ùå Coverage gap detected!")
        gap = len(text) - total_chunk_size
        print(f"   Missing: {gap:,} characters")
    
    return chunks

def chunk_text_bulletproof(text: str, prefer_ai: bool = True, model: str = "gpt-4.1-nano") -> list:
    """Main function to chunk text with bulletproof 100% coverage.
    
    Args:
        text: The full text to chunk
        prefer_ai: Whether to try AI analysis first (fallback to Python)
        model: AI model to use for analysis
        
    Returns:
        List of chunks with guaranteed 100% coverage
    """
    print("üöÄ BULLETPROOF CHUNKING SYSTEM")
    print("=" * 60)
    
    analysis = {}
    
    # Step 1: Try AI analysis if preferred
    if prefer_ai:
        print("Step 1: AI Pattern Detection...")
        analysis = ai_chapter_analyzer(text, model)
    
    # Step 2: Fallback to Python if AI fails or not preferred
    if not analysis:
        print("Step 2: Python Pattern Detection...")
        analysis = python_pattern_detector(text)
    
    if not analysis:
        print("‚ùå Both AI and Python analysis failed!")
        return []
    
    # Step 3: Create bulletproof chunks
    print("\nStep 3: Bulletproof Chunking...")
    chunks = bulletproof_chunker(text, analysis)
    
    if not chunks:
        print("‚ùå Chunking failed!")
        return []
    
    print(f"\nüèÜ SUCCESS: Created {len(chunks)} chunks with bulletproof coverage!")
    return chunks

if __name__ == "__main__":
    # Test the chunking system
    try:
        with open('test_data/pride_and_prejudice_full.txt', 'r') as f:
            test_text = f.read()
        
        chunks = chunk_text_bulletproof(test_text)
        
        if chunks:
            print(f"\nüìã CHUNK SUMMARY:")
            for i, chunk in enumerate(chunks, 1):
                tokens = chunk['size'] // 4
                print(f"  Chunk {i}: {chunk['size']:,} chars (~{tokens:,} tokens) - {chunk['description']}")
        
    except FileNotFoundError:
        print("Test file not found. Run from project root directory.")